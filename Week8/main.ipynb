{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "42842c95b5bb4d7d976b7e033a71fc38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38e59eddda8d439bb707f7407909453e",
              "IPY_MODEL_b5882febbe4b47c68db1a09dfbc3b947",
              "IPY_MODEL_aec735c51a844872821d05cf73389b71"
            ],
            "layout": "IPY_MODEL_683d8c37ea434ef693c227365edc29c9"
          }
        },
        "38e59eddda8d439bb707f7407909453e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55cd79ec63314d62833b679fb7260727",
            "placeholder": "​",
            "style": "IPY_MODEL_f96543a2b6ad406092a709060898b2b3",
            "value": "Dl Completed...: 100%"
          }
        },
        "b5882febbe4b47c68db1a09dfbc3b947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1afab4b29f0b4c60af0cdd69efb97557",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e98f66cd6a34c05af945d0a72028b4c",
            "value": 5
          }
        },
        "aec735c51a844872821d05cf73389b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5860c2f755e46c89c087a5a0b2afc5f",
            "placeholder": "​",
            "style": "IPY_MODEL_a79c9de31e1d47399a12d5ec8efaaf55",
            "value": " 5/5 [00:00&lt;00:00, 12.30 file/s]"
          }
        },
        "683d8c37ea434ef693c227365edc29c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55cd79ec63314d62833b679fb7260727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f96543a2b6ad406092a709060898b2b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1afab4b29f0b4c60af0cdd69efb97557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e98f66cd6a34c05af945d0a72028b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5860c2f755e46c89c087a5a0b2afc5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a79c9de31e1d47399a12d5ec8efaaf55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFwkJB530jv2",
        "outputId": "77b96217-1dce-465d-a054-30c838247562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_cv\n",
            "  Downloading keras_cv-0.4.1-py3-none-any.whl (615 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.8/615.8 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from keras_cv) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_cv) (21.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from keras_cv) (4.6.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from keras_cv) (2022.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras_cv) (3.0.9)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (2.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (0.10.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (2.1.1)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (3.19.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (5.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (2.25.1)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (0.9.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (0.3.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv) (4.0.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from etils[epath]->tensorflow-datasets->keras_cv) (4.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[epath]->tensorflow-datasets->keras_cv) (3.11.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv) (1.57.0)\n",
            "Installing collected packages: keras_cv\n",
            "Successfully installed keras_cv-0.4.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install keras_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data pipeline and model"
      ],
      "metadata": {
        "id": "G7EtCh5O05TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_cv\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import datetime\n",
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "# from sklearn.manifold import TSNE\n",
        "\n",
        "#import mnist data\n",
        "train_ds, test_ds = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
        "\n",
        "def augment(x, y):\n",
        "    image = tf.image.random_brightness(x, max_delta=0.05)\n",
        "    return image, y\n",
        "\n",
        "def preprocess(data, noise_var=0.5):\n",
        "    # use only the images\n",
        "    data = data.map(lambda x, t: tf.cast(x, tf.dtypes.float32))\n",
        "    # normalize images\n",
        "    data = data.map(lambda x: ((x / 128.)-1.))\n",
        "    # add noise to images\n",
        "    noise = noise_var * tf.random.normal(shape=(28, 28, 1))\n",
        "    data = data.map(lambda o_img: (o_img + noise, o_img))\n",
        "    # clip values to range of [-1,1]\n",
        "    data = data.map(lambda n_img, o_img: (tf.clip_by_value(n_img, clip_value_min=-1, clip_value_max=1), o_img))\n",
        "    # cache, shuffle, batch, prefetch\n",
        "    data = data.cache()\n",
        "    data = data.shuffle(2000)\n",
        "    data = data.batch(32)\n",
        "    data = data.prefetch(tf.data.AUTOTUNE)\n",
        "    return data\n",
        "\n",
        "class CNN_Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, dropout_rate):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # Img size: 28x28x1\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=4, kernel_size=3, strides=(2, 2), padding='same',\n",
        "                                            activation='relu', kernel_regularizer=tf.keras.regularizers.L2)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        # Img size: 14x14x4\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=8, kernel_size=3, strides=(2, 2), padding='same',\n",
        "                                            activation='relu', kernel_regularizer=tf.keras.regularizers.L2)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.batch_norm2 = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "        # Img size: 7x7x8\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        # Img size: 1x1x392\n",
        "        self.batch_norm1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "        self.out = tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.L2)\n",
        "        # Img size: 10\n",
        "\n",
        "    def call(self, x, training):\n",
        "        #print(x.shape)\n",
        "        x = self.conv1(x, training=training)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        #print(x.shape)\n",
        "        x = self.conv2(x, training=training)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        #print(x.shape)\n",
        "        x = self.flatten(x)\n",
        "        #print(x.shape)\n",
        "        x = self.batch_norm1(x, training=training)\n",
        "        x = self.out(x)\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CNN_Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, dropout_rate):\n",
        "        super(CNN_Decoder, self).__init__()\n",
        "        # Restore dimensionality\n",
        "        self.dim_restore = tf.keras.layers.Dense(392, activation='relu', kernel_regularizer=tf.keras.regularizers.L2)\n",
        "        # Img size: 1x1x3136\n",
        "        self.reshape = tf.keras.layers.Reshape((7, 7, 8))\n",
        "        # Img size: 7x7x8\n",
        "        self.convTranspose1 = tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=3, strides=(2, 2), padding='same',\n",
        "                                                              activation='relu', kernel_regularizer=tf.keras.regularizers.L2)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        # Img size: 14x14x4\n",
        "        self.convTranspose2 = tf.keras.layers.Conv2DTranspose(filters=4, kernel_size=3, strides=(2, 2), padding='same',\n",
        "                                                              activation='relu', kernel_regularizer=tf.keras.regularizers.L2)\n",
        "        self.dropout4 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.batch_norm2 = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "        # Img size: 28x28x4\n",
        "        self.out = tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=(1, 1),\n",
        "                                          padding='same')\n",
        "        # Img size: 28x28x1\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.dim_restore(x)\n",
        "        #print(x.shape)\n",
        "        x = self.reshape(x)\n",
        "        #print(x.shape)\n",
        "        x = self.convTranspose1(x, training=training)\n",
        "        x = self.dropout3(x, training=training)\n",
        "        #print(x.shape)\n",
        "        x = self.convTranspose2(x, training=training)\n",
        "        x = self.dropout4(x, training=training)\n",
        "        x = self.batch_norm2(x, training=training)\n",
        "        #print(x.shape)\n",
        "        x = self.out(x)\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "\n",
        "class CNN_Autoencoder(tf.keras.Model):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(CNN_Autoencoder, self).__init__()\n",
        "        # Layers\n",
        "        self.encoder = CNN_Encoder(dropout_rate)\n",
        "        self.decoder = CNN_Decoder(dropout_rate)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        # Loss function\n",
        "        self.loss_function = tf.keras.losses.MeanSquaredError()\n",
        "        # Metrics\n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                             tf.keras.metrics.Accuracy(name=\"accuracy\")]\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.encoder(x, training=training)\n",
        "        x = self.decoder(x, training=training)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "\n",
        "    def train_step(self, data):\n",
        "        img, label = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self(img, training=True)\n",
        "            loss = self.loss_function(label, output)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # update loss metric\n",
        "        self.metrics[0].update_state(loss)\n",
        "        for metric in self.metrics[1:]:\n",
        "            metric.update_state(label, output)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        img, label = data\n",
        "\n",
        "        output = self(img, training=False)\n",
        "        loss = self.loss_function(label, output)\n",
        "\n",
        "        # update loss metric\n",
        "        self.metrics[0].update_state(loss)\n",
        "        for metric in self.metrics[1:]:\n",
        "            metric.update_state(label, output)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def predict(self, data):\n",
        "        img, label = data\n",
        "        return self(img, training=False)\n",
        "\n",
        "def metric_before_epoch(model, train_ds, test_ds, train_summary_writer, test_summary_writer):\n",
        "    print(\"\\n\")\n",
        "    print(\"Loss and accuracy before training:\")\n",
        "\n",
        "    for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
        "        metrics = model.test_step(data)\n",
        "\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            for metric in model.metrics:\n",
        "                tf.summary.scalar(f\"{metric.name}\", metric.result(), step=1)\n",
        "\n",
        "    # print the metrics\n",
        "    print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "    # reset all metrics\n",
        "    model.reset_metrics()\n",
        "\n",
        "    for data in tqdm.tqdm(test_ds, position=0, leave=True):\n",
        "        metrics = model.test_step(data)\n",
        "\n",
        "        # logging the validation metrics to the log file which is used by tensorboard\n",
        "        with test_summary_writer.as_default():\n",
        "            for metric in model.metrics:\n",
        "                tf.summary.scalar(f\"{metric.name}\", metric.result(), step=1)\n",
        "\n",
        "    print([f\"test_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "    # reset all metrics\n",
        "    model.reset_metrics()\n",
        "    print(\"\\n\")\n",
        "\n",
        "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "\n",
        "        # Training\n",
        "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
        "            metrics = model.train_step(data)\n",
        "\n",
        "            with train_summary_writer.as_default():\n",
        "                for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        # print the metrics\n",
        "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics()\n",
        "\n",
        "        # Validation\n",
        "        for data in test_ds:\n",
        "            metrics = model.test_step(data)\n",
        "\n",
        "            # logging the validation metrics to the log file which is used by tensorboard\n",
        "            with test_summary_writer.as_default():\n",
        "                for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        print([f\"test_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics()\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "42842c95b5bb4d7d976b7e033a71fc38",
            "38e59eddda8d439bb707f7407909453e",
            "b5882febbe4b47c68db1a09dfbc3b947",
            "aec735c51a844872821d05cf73389b71",
            "683d8c37ea434ef693c227365edc29c9",
            "55cd79ec63314d62833b679fb7260727",
            "f96543a2b6ad406092a709060898b2b3",
            "1afab4b29f0b4c60af0cdd69efb97557",
            "9e98f66cd6a34c05af945d0a72028b4c",
            "e5860c2f755e46c89c087a5a0b2afc5f",
            "a79c9de31e1d47399a12d5ec8efaaf55"
          ]
        },
        "id": "FI9Cp_3r0k3E",
        "outputId": "9cadbda2-787b-42d7-c1ad-8b3faef205c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n",
            "Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to ~/tensorflow_datasets/mnist/3.0.1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42842c95b5bb4d7d976b7e033a71fc38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset mnist downloaded and prepared to ~/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "j44SPWcT1HMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################### Set paths ################\n",
        "if os.path.exists('/content/drive/MyDrive/logs/'):\n",
        "    from google.colab import drive\n",
        "    path = '/content/drive/MyDrive/logs/Autoencoder'\n",
        "    drive.mount(path)\n",
        "else:\n",
        "    path = 'C:/Users/accou/OneDrive/Desktop/Arbeit/Studium/Master/Semester03/IANNwTF/Week08/logs'\n",
        "\n",
        "################## Preprocess the data ############\n",
        "# determine noisiness\n",
        "NOISE = 1\n",
        "train_data = preprocess(train_ds, noise_var=NOISE)\n",
        "# augment data\n",
        "train_data = train_data.map(augment)\n",
        "test_data = preprocess(test_ds, noise_var=NOISE)\n",
        "\n",
        "# show noisy image\n",
        "for n_img, o_img in train_data.take(1):\n",
        "    print(\"Tensor shape = \", n_img.shape, o_img.shape)\n",
        "\n",
        "    x = np.asarray(n_img[0, :, :, 0])\n",
        "    t = np.asarray(o_img[0, :, :, 0])\n",
        "    plt.imshow(x)\n",
        "    plt.show()\n",
        "    #plt.savefig('preprocessed_noisy_img.png', format='png')\n",
        "    plt.imshow(t)\n",
        "    plt.show()\n",
        "    #plt.savefig('preprocessed_original_img.png', format='png')\n",
        "\n",
        "# create model\n",
        "DROPOUT=0.2\n",
        "model = CNN_Autoencoder(dropout_rate=DROPOUT)\n",
        "\n",
        "# Define where to save the log\n",
        "config_name = \"Autoencoder\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "train_log_path = f\"{path}/{config_name}/{current_time}/train\"\n",
        "test_log_path = f\"{path}/{config_name}/{current_time}/test\"\n",
        "\n",
        "# log writer for training metrics\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "\n",
        "# log writer for validation metrics\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
        "\n",
        "metric_before_epoch(model=model,\n",
        "                train_ds=train_data,\n",
        "                test_ds=test_data,\n",
        "                train_summary_writer=train_summary_writer,\n",
        "                test_summary_writer=test_summary_writer)\n",
        "\n",
        "training_loop(model=model,\n",
        "                train_ds=train_data,\n",
        "                test_ds=test_data,\n",
        "                epochs=10,\n",
        "                train_summary_writer=train_summary_writer,\n",
        "                test_summary_writer=test_summary_writer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "IjN27F_t0qx_",
        "outputId": "28a33603-5d79-49db-e762-fe4f5c29948b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor shape =  (32, 28, 28, 1) (32, 28, 28, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYVklEQVR4nO3deXjU1bkH8O+bjUACSEAwSGSPClVAIyBStaJAsS1wXa5UvVgtuFYUe1trvVXb2sttBXHBBZUWVOChVZS6oVItUhQJmyyxJsSwhLAlYTHBkOXtHxm80ea8vziTzEx7vp/nyZNkvnPmdzKZNzOZ8zvniKqCiP79JcS6A0QUHSx2Ik+w2Ik8wWIn8gSLncgTSdE8WIq00lSkhd2+LsPdti7gz1bS/oqwjwsAkpLizGrSk822rTsfMfPK0tZmnnyo2sxRV+eMtLrGbGr9XACgR4/ax/aU9VgEgIQq9+8EAKrbuh+wybvtx2r1Ce5jVx8sQ01lhTSWRVTsIjIawEMAEgE8rarTrOunIg1DZETYxzs8aqgzO5re6M/3hY5Pvx/2cQEg6cSTnFnpOV3NtqfdutHMc+cNMPPMpSVmLhXuPyY1JbvNttbPBQA1RdvN3FfWYxEA2hZVmvmuc9OdWdffrrTbThzmzArnznBmYb+MF5FEALMAfBtAPwATRKRfuLdHRC0rkv/ZBwMoUNVCVT0KYCGAsc3TLSJqbpEU+4kAdjT4fmfosi8RkckikisiudWoiuBwRBSJFn83XlVnq2qOquYko1VLH46IHCIp9mIAWQ2+7xa6jIjiUCTFvhpAXxHpKSIpAK4AsKR5ukVEzU0imfUmImMAzET90NscVb3fun6bzlmafdntzrwy0z5e919ENnwWK0W/OtvM22+123f4Q/g/95Fxg8289Usfhn3bTbHtl+6fPej3mdQ9y8wLJnUz8x53t9zjZemu9Wbee9kPzLzP1euasztfWKXLcEjLmn+cXVVfA/BaJLdBRNHB02WJPMFiJ/IEi53IEyx2Ik+w2Ik8wWIn8kRU57MnVdbh+DWfOfOlLz1rth/1i4HOrOwH9lh2xu9bbsy172r7NOCiN+z2GQvWmnkk6/9WtUs0c3smPZCQmmpfIbuHGVtj6UuKV5ttBz3hnsoJAH2Hf2rmrxhj4RflfddsmzBih5mP6up+LAJAH9jj6LXfOsOZpawpMNtWDj/ZmekK9/3NZ3YiT7DYiTzBYifyBIudyBMsdiJPsNiJPBHVoTdUHAE+dK+0+o2HbzKbd7nAvaxVYnXsNqjMP8tebqsH7GG/NwKmSwYN81iOm2cfe+sD9iqpdan2/dr3llVfu0/H5FbZw4JZv7JXWf3OhH1m3vPPk5xZnz72qrstLWnFJmcmHTuYbVu95h6yFHWvastndiJPsNiJPMFiJ/IEi53IEyx2Ik+w2Ik8wWIn8kRUx9lrO6WhbKx7KmrV6fbOl0nT3OPR7cPuVeSClhUOGicfc8q5AUc49DV71HS16fbWwtk3tNxS04kRTd4FFvc73sxPznHvbqu59hTWnT+zp9f2mFdk5q+uthdd/vboK5xZZde2ZtuUN/aYuQuf2Yk8wWIn8gSLncgTLHYiT7DYiTzBYifyBIudyBMRbdn8dbWTDB0iI6J2vIb23GqPm3Z52J47/ckzOc4s+7rcsPrku8RT+9pX2FdmxnXdTzDzN/78vDOLZI2AWDs6yv1YXLvyERw+uLP5t2wWkSIAhwHUAqhRVXcviCimmuMMum+p6v5muB0iakH8n53IE5EWuwJ4U0TWiMjkxq4gIpNFJFdEcqthr9VGRC0n0pfxw1W1WEQ6A3hLRD5W1eUNr6CqswHMBurfoIvweEQUpoie2VW1OPR5L4DFAAY3R6eIqPmFXewikiYibY99DWAkAPf6uEQUU5G8jO8CYLGIHLud+apqb06c3hp1OYOc8f7+9gbCVcZy2ln32+PkR9uZcaBIxtLLX7XHkztcnG/m+Y8OMfPC/3jSmfVc0uhbKV94ZfRDZj61h70VdpCKS919Lx5Ta7bNvta+X7C/1Ix7vXmdM+uLNWbb0kn2z/3ZRe6txwGg7evpZh7JFuIpS92PRWvd+LCLXVULAQwItz0RRReH3og8wWIn8gSLncgTLHYiT7DYiTwRV1Nct863px1m31DozOpO7m621dXuraIBILGdPTZXe6jllnMOUvP2SWa+q9y9kHbyB/ayxJnT7SHLxOPsRbprDxw0cx3mHrDJn5hitu13z3Yzf3XtUjO3RDrFNSE11czrPv887NuWJHuQbN+1Zzmzv7/wICr37Wh0iiuf2Yk8wWIn8gSLncgTLHYiT7DYiTzBYifyBIudyBNxNc4eJGHAqc6sbkOe2XbbotPMvPeUfWZeU7LbzC2FAecP9Pq+veVz6Q/t6ZYHTnH/DrN/ucVs2+cv7m2NAeDhrqvNPO+ovc32qSltzNyy+ajdt/uLx5j5/J7vhH3seF5qOqmH+7yLlcXP42DVbo6zE/mMxU7kCRY7kSdY7ESeYLETeYLFTuQJFjuRJ5pjY8eoCRpLN9sW2Ev71pTY892Hbqh2Zvcdvzng6PY4+uUr7HMPlvZ63MzPvcm9XPS4VQVm2/l3XGzmeNoeZw8aR+///pXObPPZ7i2VAWDC+mvN/MHTFpn5yEsmOrPtt9eZbdN/aD9eOj5tLwUddG6E1b74p/b24ikH3edVVP8p2ZnxmZ3IEyx2Ik+w2Ik8wWIn8gSLncgTLHYiT7DYiTzxLzWfPTG7tzOr6WSPiyZ/usfMS0f0MPP2z33gzD5deLrZdvVw95bKANA+wd6quiX1efcaM09JqTHzzs/YfW/1unucvnK8vRX1e7Ps+23Qr28y83V3P+bMzt80zmzbamSRmUdsqPsxI2s+ttsOyHZGH2x+EocqdoU3n11E5ojIXhHZ1OCyDBF5S0TyQ5+NndOJKB405WX8HwCM/spldwJYpqp9ASwLfU9EcSyw2FV1OYCyr1w8FsDc0NdzAdiviYgo5sI9N76LqpaEvt4NoIvriiIyGcBkAEhF+OuREVFkIn43Xuvf4XO+y6eqs1U1R1VzktEq0sMRUZjCLfY9IpIJAKHPe5uvS0TUEsIt9iUAjs0fnAjg5ebpDhG1lMD/2UVkAYDzAXQSkZ0A7gEwDcAiEbkOwDYAlzflYFUnpSH/p+6x1VMeKzfb593h3kO94wfuebwA0HGlve57bXJPM1+6yz0n/aZie6/uSMfRh91+g5m/N8M9npwo9t/z3gFr1le92cPM0/LsfevH5rlf9E1722wa6P+mPmXmj5R3d2YHX+pqtu2MIjMvnWTPV/8sy4zRa5778Xj0nG+YbRPfXesO1b0vfGCxq+oERxT+2TFEFHU8XZbIEyx2Ik+w2Ik8wWIn8gSLncgT/1JTXJOyujmzy9760Gy74BR7qMUaWgOAb107yZlNeWSB2XZc2mdmHun2wDv+5B6qybp0kzMDgn/uQb+xp5F2fnSlmW/7pXuIauHVM822/7XhGjM/vXOJmZd/3z1UW1O03WwbZO8t9nLPQfeLJeh3Yj1eVukyHNIybtlM5DMWO5EnWOxEnmCxE3mCxU7kCRY7kSdY7ESeiKstm8sn2tMGO8x1b3MbNI6e2KmjmY/+3lVm3v/xj5xZ0Dh6kKDpkum77OWcv3mSeyx95U/s8eCg7aQjGS8GgDYDv7p84f+75KUpZtut//mEmfd99kYz71XkfrwEjWX3e8w+v6D9VnvLZxnU38x1nXub76DzLsqudT9eal92L3nOZ3YiT7DYiTzBYifyBIudyBMsdiJPsNiJPMFiJ/JEXM1nr77wTLN9RWaKMzvuWfeYalMkdTvRzLW1ezebpKcrzbY1l9aaee2+fWYeRM9xj8vOXzjLbHvxXT8286D79dDr7m20ASBhTidn9reH7C2Zg8abb87/xMxn9XVvbRz0+67ZWWzmQapH5ph5yVD3Y7nnQnt78cIrnbutYfsTM/B58Q7OZyfyGYudyBMsdiJPsNiJPMFiJ/IEi53IEyx2Ik/E1Tj7+C32ePPifseHfWw5055fnLj3oJnX7NgZ9rGD3FrwsZn/5mcTzbztS+ucmVYfNdsmDOxn5jsvPM7Ml095wMzPeOU2Z3bD8HfMtq/84gIzb7N4lZnHs4LnBjmzrM721uWtRhY5s4jWjReROSKyV0Q2NbjsXhEpFpH1oY8xQbdDRLHVlJfxfwAwupHLH1TVgaGP15q3W0TU3AKLXVWXA3CvLURE/xIieYPuFhH5KPQyv4PrSiIyWURyRSS3GlURHI6IIhFusT8OoDeAgQBKAEx3XVFVZ6tqjqrmJMM9mYSIWlZYxa6qe1S1VlXrADwFYHDzdouImltYxS4imQ2+HQ/A3heYiGIucN14EVkA4HwAnURkJ4B7AJwvIgMBKIAiANc35WCSnIykE9zziJ96yF4/vRPcc6t12AD74Cs3mHFtwDrfMMbZk05wzy8GgCOnZ5n5w33sQ7f5pj1WXvEd95ht0Fh03fotZp5V0cvMv3uhvd5+xzWJzqzwTPdcdwCoad3ocHGTHRnrfsGZctBeiz/x3bVmXjBjqJn3mepevx0A+lzlPjdi63x7Hr+9goBbYLGr6oRGLn4mzOMRUYzwdFkiT7DYiTzBYifyBIudyBMsdiJPRHXL5sTedWg754gzrxvhHo4AADm1rzvbUWq2tQda7C10g9SWHzDzoqvsZYuvfeBzM3/9V+5lhwEg/Y8tN9VTau2tiVec/pKZ9yya7Mye7GYvUz1qvvux0hRtN+x2Zlvu7my2zX7Xvu2gobUguxa7pxafNMs9XBkJPrMTeYLFTuQJFjuRJ1jsRJ5gsRN5gsVO5AkWO5EnojrOfuRAKja+coozT51oL2tdfrZ7qufJD9o/imbZU2AlYAqsedtV9nJbTwx71sxnfnecme+/0v6bnP5HMza1+usJZr6pKMPMz95wiZm37uTezjpoS+bt9wwz85PuW2nmY1/PdWZ7po812wapPf8MMw+aItt1vHtqcfk19lRv5xpwAfjMTuQJFjuRJ1jsRJ5gsRN5gsVO5AkWO5EnWOxEnojqls05A1L1w6XuZZXH9DvPbF97wL2tsp5jj9nK39bbnWtB8hd7PvueF7ubeedH7fHkT57JcWbZ17nHmgHgvI/sOeMXtrW3BPj+Cz8y894/Dn/e9+7b7HH2zPfsbbY10f1ctujF2Wbby7vZY91d3m9n5nvOPmTmM4rcc/kn/7d7m2sAqEl1/1xbXn0QFft3hLdlMxH9e2CxE3mCxU7kCRY7kSdY7ESeYLETeYLFTuSJqM5nzys+Hmf9/EZnnnHAXkfckrSx0Myrl9nbJieM2BH2sQ9eaW/f2/4Ce6y5w4X2nPLbC/LMfP4+95jvikeGmG2XdnrSzIFkM41kHD1I+0/t1f4LL7PHupMPurd8vvQK9+MQABJgn5cRNI7+6f/a4/RTe7iz6YWPmW3v+96VziypstaZBT6zi0iWiLwjIltEZLOITAldniEib4lIfuhzuHPqiSgKmvIyvgbAHaraD8BQADeLSD8AdwJYpqp9ASwLfU9EcSqw2FW1RFXXhr4+DCAPwIkAxgKYG7raXAD22kpEFFNf6w06EekBYBCAVQC6qGpJKNoNoIujzWQRyRWR3JrPKyLoKhFFosnFLiLpAF4AcJuqfundCa2fTdPojBpVna2qOaqak5SaFlFniSh8TSp2EUlGfaE/r6ovhi7eIyKZoTwTwN6W6SIRNYfAKa4iIqj/n7xMVW9rcPnvAJSq6jQRuRNAhqr+xLqt7NNa68Mv93TmM88abvaltrzczC1HlrqPCwCtR30a9m0HKbrfHobZdM2jZn510UVmnvdH9/LcJ8y0p8cmGttgA4AcOGzmNSXubZFjLambe2rxZW+vNtsuOKVrc3fnS5bucg/tBS2xnf+Qe6i35HczUbW98SmuTRlnPwfA1QA2isixHt4FYBqARSJyHYBtAC5vwm0RUYwEFruqrgDgOjthRPN2h4haCk+XJfIEi53IEyx2Ik+w2Ik8wWIn8kRUl5JuJxk6RGLzBn75q/Z4coeL8+0bGHyaMyrrn242zfh9+FN3ASDhdPc4OgB887l1zmzZ3pPNtq2Tqs18fBf3bQPAolPt6bnW1sYpm+1pxZJiT6+tKd5l5okd3dtN15aWmW2D7LvRPneifIB7qikAZN/wYdjHvrXgY2c2dWwB8jce4VLSRD5jsRN5gsVO5AkWO5EnWOxEnmCxE3mCxU7kCW/G2Y+MG2zmJUMTzbzPc+659HWb3OOeTVH8Yn8zXzdknpkni913y5uV9lj2lHVXmPlJl2008/Fb9jmzFyfZ8/SDttkunWSPdZee4R7rzr7RHudO6tXDzLdPb2PmXcdvMfNti9znbdzcf7nZ9pX+7oWcV+kyHNIyjrMT+YzFTuQJFjuRJ1jsRJ5gsRN5gsVO5AkWO5Enoj/OnjjSfYU6ew5wvNr/52wzT3/yODP/6+zZER3/4nPGOrOqHh3NtonvrI3o2DGVEHB+gfF42nnXMLNpZXd7nn/29fa680HbeO+9yH372ddvMtvmP93Pme36n1moKizmODuRz1jsRJ5gsRN5gsVO5AkWO5EnWOxEnmCxE3kicBdXEckCMA9AFwAKYLaqPiQi9wKYBODYhOW7VPU167a0fRt8ft6Zzjwtzz33GQBqC8LfQz2xby8z7/KsfexdQ937lN9/6stm29qZrk1w65024yYzb3ehvQf6ocvca7e32W2fR1H+G3tO+NRxS8x8cb/jzXzYhqPO7MPv9Tbb1u2313b/+yx7Pf3Ckc84s1EB268ndc8y89rkFDNP222P0+OQu/QqRw8wm/a52j0Xv1SPOLOm7M9eA+AOVV0rIm0BrBGRt0LZg6r6QBNug4hirCn7s5cAKAl9fVhE8gCc2NIdI6Lm9bX+ZxeRHgAGAVgVuugWEflIROaISKNr5YjIZBHJFZHc6qMVEXWWiMLX5GIXkXQALwC4TVUPAXgcQG8AA1H/zD+9sXaqOltVc1Q1JzklrRm6TEThaFKxi0gy6gv9eVV9EQBUdY+q1qpqHYCnANgrOhJRTAUWu4gIgGcA5KnqjAaXZza42ngA9lQdIoqpwCmuIjIcwHsANgKoC118F4AJqH8JrwCKAFwfejPPKZZLSQepO2+Qme8d1NqZdZ1tL3lcV1lp5jvutqdbHulaY+aZy91/s9su/MBsm9jJngJbu7/UzEum2n3PnLHSmW27z27b62F7ie7AbZfFGPIMeNwnDHRPIwWAuvX2UtGxYi0l3ZR341cAaKyxOaZORPGFZ9AReYLFTuQJFjuRJ1jsRJ5gsRN5gsVO5ImmzHprNrWd0lA21j2lMmPO+2b7I0t7OrNdpe3Ntn2m21MOC66x/+5l/8A9XlznTOoFLSuc9Wv3bQPA0dFnmXlqyUFn1nq5PQW14lx7am+Qru+4jw3Un4Th0rbIHusOHEcPYoylV1wyxGya9sIqMw/y2LYVZn7DVT9yZjsvcJ/TAQAZee5HXN2b7vMq+MxO5AkWO5EnWOxEnmCxE3mCxU7kCRY7kSdY7ESeiOqWzSKyD8C2Bhd1ArA/ah34euK1b/HaL4B9C1dz9q27qjZ6ckVUi/2fDi6Sq6o5MeuAIV77Fq/9Ati3cEWrb3wZT+QJFjuRJ2Jd7LNjfHxLvPYtXvsFsG/hikrfYvo/OxFFT6yf2YkoSljsRJ6ISbGLyGgR+buIFIjInbHog4uIFInIRhFZLyK5Me7LHBHZKyKbGlyWISJviUh+6HOje+zFqG/3ikhx6L5bLyJjYtS3LBF5R0S2iMhmEZkSujym953Rr6jcb1H/n11EEgF8AuAiADsBrAYwQVXjYtV9ESkCkKOqMT8BQ0TOBfAZgHmq+o3QZb8FUKaq00J/KDuo6k/jpG/3Avgs1tt4h3Yrymy4zTiAcQCuQQzvO6NflyMK91ssntkHAyhQ1UJVPQpgIYCxMehH3FPV5QC+ulzLWABzQ1/PRf2DJeocfYsLqlqiqmtDXx8GcGyb8Zjed0a/oiIWxX4igB0Nvt+J+NrvXQG8KSJrRGRyrDvTiC4NttnaDaBLLDvTiMBtvKPpK9uMx819F87255HiG3T/bLiqngHg2wBuDr1cjUta/z9YPI2dNmkb72hpZJvxL8Tyvgt3+/NIxaLYiwFkNfi+W+iyuKCqxaHPewEsRvxtRb3n2A66oc97Y9yfL8TTNt6NbTOOOLjvYrn9eSyKfTWAviLSU0RSAFwBYEkM+vFPRCQt9MYJRCQNwEjE31bUSwBMDH09EcDLMezLl8TLNt6ubcYR4/su5tufq2rUPwCMQf078lsB/DwWfXD0qxeADaGPzbHuG4AFqH9ZV4369zauA9ARwDIA+QDeBpARR317FvVbe3+E+sLKjFHfhqP+JfpHANaHPsbE+r4z+hWV+42nyxJ5gm/QEXmCxU7kCRY7kSdY7ESeYLETeYLFTuQJFjuRJ/4Btp1zg3WbEgEAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Loss and accuracy before training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [01:09<00:00, 26.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 0.9225881695747375', 'train_accuracy: 0.0']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:10<00:00, 28.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test_loss: 0.9226378202438354', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [02:19<00:00, 13.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 0.289789080619812', 'train_accuracy: 1.7006802011110267e-07']\n",
            "['test_loss: 0.17167478799819946', 'test_accuracy: 7.653061402379535e-07']\n",
            "\n",
            "\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [02:21<00:00, 13.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_loss: 0.15170976519584656', 'train_accuracy: 2.5297620140918298e-06']\n",
            "['test_loss: 0.15351280570030212', 'test_accuracy: 0.0']\n",
            "\n",
            "\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 1836/1875 [02:15<00:02, 13.28it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent space analysis"
      ],
      "metadata": {
        "id": "9hog-q0o1M-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# initialize encoder as model_E\n",
        "model_E = model.encoder\n",
        "\n",
        "def preprocess_w_labels(data, noise_var=0.5):\n",
        "    # use only the images\n",
        "    data = data.map(lambda x, t: (tf.cast(x, tf.dtypes.float32), t))\n",
        "    # normalize images\n",
        "    data = data.map(lambda x, t: (((x / 128.)-1.), t))\n",
        "    \n",
        "    # add noise to images\n",
        "    noise = noise_var * tf.random.normal(shape=(28, 28, 1))\n",
        "    data = data.map(lambda x, t: (x + noise, t))\n",
        "    # clip values to range of [-1,1]\n",
        "    data = data.map(lambda x, t: (tf.clip_by_value(x, clip_value_min=-1, clip_value_max=1), t))\n",
        "    # cache, shuffle, batch, prefetch\n",
        "    data = data.cache()\n",
        "    data = data.shuffle(2000)\n",
        "    data = data.batch(32)\n",
        "    data = data.prefetch(tf.data.AUTOTUNE)\n",
        "    return data\n",
        "\n",
        "#create labeled dataset\n",
        "val_data = preprocess_w_labels(test_ds, noise_var=1)\n",
        "\n",
        "# take 32 datasets (aka 1024 images because we have a batch size of 32) \n",
        "var = val_data.take(32)\n",
        "img = []\n",
        "label = []\n",
        "for x, t in var:\n",
        "    img.append(model_E(x))\n",
        "    label.append(t)\n",
        "\n",
        "# concat on batch axis and convert to numpy array\n",
        "img = tf.concat(img, axis=0)\n",
        "img = img.numpy()\n",
        "label = tf.concat(label, axis=0)\n",
        "label = label.numpy()\n",
        "\n",
        "# build t-SNE\n",
        "tsne_img = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(img)\n",
        "tsne_img.shape"
      ],
      "metadata": {
        "id": "L1BRuA640yol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib\n",
        "# plot t-SNE\n",
        "colors = ['red','green','blue','purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
        "plt.scatter(tsne_img[:,0], tsne_img[:,1], c=label, cmap=matplotlib.colors.ListedColormap(colors))\n",
        "cb = plt.colorbar()\n",
        "loc = np.arange(0,max(label),max(label)/float(len(colors)))\n",
        "cb.set_ticks(loc)\n",
        "cb.set_ticklabels(['0','1','2','3','4','5','6','7','8','9'])"
      ],
      "metadata": {
        "id": "m_kJLsjV01OL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}